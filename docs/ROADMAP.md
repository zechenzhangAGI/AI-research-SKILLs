# üó∫Ô∏è Roadmap

## Vision

Build the most comprehensive open-source library of AI research skills, enabling AI agents to autonomously conduct experiments from hypothesis to deployment.

**Target**: 70 comprehensive skills by month 6

## Progress Overview

| Metric | Current | Target |
|--------|---------|--------|
| **Skills** | **70** (high-quality, standardized YAML) | 70 ‚úÖ |
| **Avg Lines/Skill** | **420 lines** (focused + progressive disclosure) | 200-500 lines |
| **Documentation** | **~115,000 lines** total (SKILL.md + references) | 100,000+ lines |
| **Gold Standard Skills** | **58** with comprehensive references | 50+ |
| **Coverage** | Architecture, Tokenization, Fine-Tuning, Data Processing, Post-Training, Safety, Distributed, Infrastructure, Optimization, Evaluation, Inference, Agents, RAG, Multimodal, MLOps, Observability, Prompt Engineering, Emerging Techniques | Full Lifecycle ‚úÖ |

## Development Phases

### ‚úÖ Phase 1: Model Architecture (COMPLETE - 5 skills)
**Status**: Core model architectures covered

**Completed Skills**:
- ‚úÖ **Megatron-Core** - NVIDIA's framework for training 2B-462B param models
- ‚úÖ **LitGPT** - Lightning AI's 20+ clean LLM implementations
- ‚úÖ **Mamba** - State-space models with O(n) complexity
- ‚úÖ **RWKV** - RNN+Transformer hybrid, infinite context
- ‚úÖ **NanoGPT** - Educational GPT in ~300 lines by Karpathy

### ‚úÖ Phase 2: Tokenization (COMPLETE - 2 skills)
**Status**: Essential tokenization frameworks covered

**Completed Skills**:
- ‚úÖ **HuggingFace Tokenizers** - Rust-based, BPE/WordPiece/Unigram
- ‚úÖ **SentencePiece** - Language-independent tokenization

### ‚úÖ Phase 3: Fine-Tuning (COMPLETE - 4 skills)
**Status**: Core fine-tuning frameworks covered

**Completed Skills**:
- ‚úÖ **Axolotl** - YAML-based fine-tuning with 100+ models
- ‚úÖ **LLaMA-Factory** - WebUI no-code fine-tuning
- ‚úÖ **Unsloth** - 2x faster QLoRA fine-tuning
- ‚úÖ **PEFT** - Parameter-efficient fine-tuning with LoRA, QLoRA, DoRA, 25+ methods

### ‚úÖ Phase 4: Data Processing (COMPLETE - 2 skills)
**Status**: Distributed data processing covered

**Completed Skills**:
- ‚úÖ **Ray Data** - Distributed ML data processing
- ‚úÖ **NeMo Curator** - GPU-accelerated data curation

### ‚úÖ Phase 5: Post-Training (COMPLETE - 4 skills)
**Status**: RLHF and alignment techniques covered

**Completed Skills**:
- ‚úÖ **TRL Fine-Tuning** - Transformer Reinforcement Learning
- ‚úÖ **GRPO-RL-Training** - Group Relative Policy Optimization (gold standard)
- ‚úÖ **OpenRLHF** - Full RLHF pipeline with Ray + vLLM
- ‚úÖ **SimPO** - Simple Preference Optimization

### ‚úÖ Phase 6: Safety & Alignment (COMPLETE - 4 skills)
**Status**: Core safety frameworks covered

**Completed Skills**:
- ‚úÖ **Constitutional AI** - AI-driven self-improvement via principles
- ‚úÖ **LlamaGuard** - Safety classifier for LLM inputs/outputs
- ‚úÖ **NeMo Guardrails** - Programmable guardrails with Colang
- ‚úÖ **Prompt Guard** - Meta's 86M prompt injection & jailbreak detector

### ‚úÖ Phase 7: Distributed Training (COMPLETE - 5 skills)
**Status**: Major distributed training frameworks covered

**Completed Skills**:
- ‚úÖ **DeepSpeed** - Microsoft's ZeRO optimization
- ‚úÖ **PyTorch FSDP** - Fully Sharded Data Parallel
- ‚úÖ **Accelerate** - HuggingFace's distributed training API
- ‚úÖ **PyTorch Lightning** - High-level training framework
- ‚úÖ **Ray Train** - Multi-node orchestration

### ‚úÖ Phase 8: Optimization (COMPLETE - 6 skills)
**Status**: Core optimization techniques covered

**Completed Skills**:
- ‚úÖ **Flash Attention** - 2-4x faster attention with memory efficiency
- ‚úÖ **bitsandbytes** - 8-bit/4-bit quantization
- ‚úÖ **GPTQ** - 4-bit post-training quantization
- ‚úÖ **AWQ** - Activation-aware weight quantization
- ‚úÖ **HQQ** - Half-Quadratic Quantization without calibration data
- ‚úÖ **GGUF** - llama.cpp quantization format for CPU/Metal inference

### ‚úÖ Phase 9: Evaluation (COMPLETE - 1 skill)
**Status**: Standard benchmarking framework available

**Completed Skills**:
- ‚úÖ **lm-evaluation-harness** - EleutherAI's standard for benchmarking LLMs

### ‚úÖ Phase 10: Inference & Serving (COMPLETE - 4 skills)
**Status**: Production inference frameworks covered

**Completed Skills**:
- ‚úÖ **vLLM** - High-throughput LLM serving with PagedAttention
- ‚úÖ **TensorRT-LLM** - NVIDIA's fastest inference
- ‚úÖ **llama.cpp** - CPU/Apple Silicon inference
- ‚úÖ **SGLang** - Structured generation with RadixAttention

### ‚úÖ Phase 10.5: Infrastructure (COMPLETE - 3 skills)
**Status**: Cloud infrastructure and orchestration covered

**Completed Skills**:
- ‚úÖ **Modal** - Serverless GPU cloud with Python-native API, T4-H200 on-demand
- ‚úÖ **SkyPilot** - Multi-cloud orchestration across 20+ providers with spot recovery
- ‚úÖ **Lambda Labs** - Reserved/on-demand GPU cloud with H100/A100, persistent filesystems

### ‚úÖ Phase 11: Agents (COMPLETE - 4 skills)
**Status**: Major agent frameworks covered

**Completed Skills**:
- ‚úÖ **LangChain** - Most popular agent framework, 500+ integrations
- ‚úÖ **LlamaIndex** - Data framework for LLM apps, 300+ connectors
- ‚úÖ **CrewAI** - Multi-agent orchestration with role-based collaboration
- ‚úÖ **AutoGPT** - Autonomous AI agent platform with visual workflow builder

### ‚úÖ Phase 12: RAG (COMPLETE - 5 skills)
**Status**: Core RAG and vector database skills covered

**Completed Skills**:
- ‚úÖ **Chroma** - Open-source embedding database
- ‚úÖ **FAISS** - Facebook's similarity search, billion-scale
- ‚úÖ **Sentence Transformers** - 5000+ embedding models
- ‚úÖ **Pinecone** - Managed vector database
- ‚úÖ **Qdrant** - High-performance Rust vector search with hybrid filtering

### ‚úÖ Phase 13: Multimodal (COMPLETE - 7 skills)
**Status**: Comprehensive multimodal frameworks covered

**Completed Skills**:
- ‚úÖ **CLIP** - OpenAI's vision-language model
- ‚úÖ **Whisper** - Robust speech recognition, 99 languages
- ‚úÖ **LLaVA** - Vision-language assistant, GPT-4V level
- ‚úÖ **Stable Diffusion** - Text-to-image generation via HuggingFace Diffusers
- ‚úÖ **Segment Anything (SAM)** - Meta's zero-shot image segmentation with points/boxes/masks
- ‚úÖ **BLIP-2** - Vision-language pretraining with Q-Former, image captioning, VQA
- ‚úÖ **AudioCraft** - Meta's MusicGen/AudioGen for text-to-music and text-to-sound

### ‚úÖ Phase 14: Advanced Optimization (COMPLETE)
**Status**: Advanced optimization techniques covered (merged into Phase 8)

**Note**: HQQ and GGUF skills have been completed and merged into Phase 8: Optimization.

### ‚úÖ Phase 15: MLOps & Observability (COMPLETE - 5 skills)
**Status**: Core MLOps and LLM observability covered

**Completed Skills**:
- ‚úÖ **MLflow** - Open-source MLOps platform for tracking experiments
- ‚úÖ **TensorBoard** - Visualization and experiment tracking
- ‚úÖ **Weights & Biases** - Experiment tracking and collaboration
- ‚úÖ **LangSmith** - LLM observability, tracing, evaluation
- ‚úÖ **Phoenix** - Open-source AI observability with OpenTelemetry tracing

### ‚úÖ Phase 16: Prompt Engineering & Advanced Applications (COMPLETE - 6 skills)
**Status**: Core prompt engineering and multi-agent tools covered

**Completed Skills**:
- ‚úÖ **DSPy** - Declarative prompt optimization and LM programming
- ‚úÖ **Guidance** - Constrained generation and structured prompting
- ‚úÖ **Instructor** - Structured output with Pydantic models
- ‚úÖ **Outlines** - Structured text generation with regex and grammars
- ‚úÖ **CrewAI** - Multi-agent orchestration (completed in Phase 11)
- ‚úÖ **AutoGPT** - Autonomous agents (completed in Phase 11)

### ‚úÖ Phase 17: Extended Multimodal (COMPLETE)
**Status**: All extended multimodal skills complete, merged into Phase 13

**Note**: BLIP-2, SAM, and AudioCraft have been completed and merged into Phase 13: Multimodal.

### ‚úÖ Phase 18: Emerging Techniques (COMPLETE - 6 skills)
**Status**: Core emerging techniques covered

**Completed Skills**:
- ‚úÖ **MoE Training** - Mixture of Experts with DeepSpeed/HuggingFace
- ‚úÖ **Model Merging** - mergekit, SLERP, and model composition
- ‚úÖ **Long Context** - RoPE extensions, ALiBi, and context scaling
- ‚úÖ **Speculative Decoding** - Medusa, Lookahead, and draft models for faster inference
- ‚úÖ **Knowledge Distillation** - MiniLLM, reverse KLD, teacher-student training
- ‚úÖ **Model Pruning** - Wanda, SparseGPT, and structured pruning

## Contributing to the Roadmap

Want to help us achieve these goals?

1. **Pick a skill from the roadmap** - Comment on [GitHub Discussions](https://github.com/orchestra-research/AI-research-SKILLs/discussions) to claim it
2. **Follow the [contribution guide](CONTRIBUTING.md)** - Use our template and quality standards
3. **Submit your PR** - We review within 48 hours

## üéâ Roadmap Complete!

All 70 skills have been completed! The library now covers the full AI research lifecycle:

1. ‚úÖ **Phase 1-10**: Core ML infrastructure (Architecture, Tokenization, Fine-Tuning, Data Processing, Post-Training, Safety, Distributed Training, Optimization, Evaluation, Inference)
2. ‚úÖ **Phase 10.5**: Infrastructure (Modal, SkyPilot, Lambda Labs)
3. ‚úÖ **Phase 11-12**: Applications (Agents, RAG)
4. ‚úÖ **Phase 13**: Multimodal (CLIP, Whisper, LLaVA, Stable Diffusion, SAM, BLIP-2, AudioCraft)
5. ‚úÖ **Phase 14-16**: Advanced (Optimization, MLOps & Observability, Prompt Engineering)
6. ‚úÖ **Phase 17-18**: Extended (Extended Multimodal, Emerging Techniques)

## Future Directions

While the 70-skill roadmap is complete, the library will continue to evolve with:
- **Updates**: Keeping existing skills current with latest versions
- **Community contributions**: Additional skills from contributors
- **Emerging tools**: New frameworks and techniques as they mature

## Philosophy

**Quality over Quantity**: Each skill must provide real value with comprehensive guidance, not just links to docs. We aim for 300+ lines of expert-level content per skill, with real code examples, troubleshooting guides, and production-ready workflows.
